{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Tour of Traditional NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The term natural language processing is a more specific term referring to the sub-field of computer science that deals with methods to analyze, model, and understand human language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP?\n",
    "\n",
    "- Natural language processing (evolved from computational linguistics) uses methods from various disciplines, such as computer science, artificial intelligence, linguistics, and data science, to enable computers to understand human language in both written and verbal forms. \n",
    "\n",
    "### Diffrence between NLP and Computational Linguistisc?\n",
    "\n",
    "> Natural language processing emphasizes its use of machine learning and deep learning techniques to complete tasks, like language translation or question answering.\n",
    " \n",
    "\n",
    "> While computational linguistics has more of a focus on aspects of language, such as syntax, semantics, and grammatical structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NLP vs NLU VS NLG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While natural language processing (NLP), natural language understanding (NLU), and natural language generation (NLG) are all related topics, they are distinct ones.\n",
    "\n",
    "- At a high level, NLU and NLG are just components of NLP.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"NLP_NLU.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Natural Language Understanding** (When you want to understand the meaning of a sentence)\n",
    "\n",
    "\n",
    "- Natural language understanding is a subset of natural language processing, which uses syntactic and semantic analysis of text and speech to determine the meaning of a sentence.\n",
    "\n",
    "\n",
    "Example 1:\n",
    "\n",
    "- Alice is swimming against the current.\n",
    "\n",
    "- The current version of the report is in the folder.\n",
    "\n",
    "\n",
    "Example 2:\n",
    "\n",
    "- I will give you a ring tomorrow.\n",
    "\n",
    "- The ring is in the folder.\n",
    "\n",
    "\n",
    "Example 3:\n",
    "\n",
    "\n",
    "- The profits increases by 10%.\n",
    "\n",
    "- The pains increase day by day.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Natural Language Generation** (When computers writes language)\n",
    "\n",
    "\n",
    "- While natural language understanding focuses on computer reading comprehension, natural language generation enables computers to write. \n",
    "\n",
    "- NLG is the process of producing a human language text response based on some data input. This text can also be converted into a speech format through text-to-speech services.\n",
    "\n",
    "- NLG tasks include: \n",
    "  \n",
    "  - generating text, \n",
    "  \n",
    "  - generating speech, \n",
    "  \n",
    "  - and generating images \n",
    "  \n",
    "  - and videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](npl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Heuristics-based NLP (Rule-based NLP)\n",
    "\n",
    "- Machine Learning NLP\n",
    "\n",
    "- Deep Learning for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristics-based NLP Examples:\n",
    "\n",
    "- Dictionary-based sentiment analysis (Lexicon-based SA)\n",
    "\n",
    "- WordNet for lexical relations\n",
    "\n",
    "- Regular Expressions\n",
    "\n",
    "- Context-free grammar\n",
    "\n",
    "### Strengths:\n",
    "\n",
    "- Rules based on domain-specific knowledge can efficiently reduce the mistakes that are sometimes very expensive.\n",
    "\n",
    "### Dis\n",
    "\n",
    "- Manually curation of feuatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Machine Learning for NLP\n",
    "\n",
    "\n",
    "### Common methods for machine learning:\n",
    "\n",
    "- Naive Bayes\n",
    "  \n",
    "- Logistic Regression\n",
    "\n",
    "- Support Vector Machine\n",
    "\n",
    "- Hidden Markov Model\n",
    "\n",
    "- Conditional Random Field\n",
    "\n",
    "\n",
    "### Three common steps for machine learning\n",
    "\n",
    "- Extracting features from texts\n",
    "\n",
    "- Using the feature representation to learn a model\n",
    "\n",
    "- Evaluating and improving the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP\n",
    "\n",
    "Convolutional Neural Network (CNN)\n",
    "  \n",
    "Sequence Models\n",
    "  \n",
    "  - Recurrent Neural Network (RNN)\n",
    "  \n",
    "  - Long-Term Short-Term Memory (LSTM)\n",
    "\n",
    "\n",
    "Strengths of Sequence Models\n",
    "\n",
    "- It reflects the fact that a sentence in language flows from one direction to another.\n",
    "\n",
    "- The model can progressively read an input text from one end to another.\n",
    "\n",
    "- The model have neural units capable of remembering what it has processed so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning\n",
    "\n",
    "- It is a technique in AI where the knowledge gained while solving one problem is applied to a different but related problem.\n",
    "\n",
    "- We can use unsupervised methods to train a transformer-based model for predicting a part of a sentence given the rest of the content.\n",
    "\n",
    "- This model can encode high-level nuances of the language, which can be applied to other relevant downstream tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers\n",
    "\n",
    "- The state-of-the-art model in major NLP tasks\n",
    "  \n",
    "- It models the textual context in a non-sequential manner.\n",
    "\n",
    "- Given a word in the input, the model looks at all the words around it and represent each word with respect to its context. This is referred to as self-attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora, Tokens, and Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AllÂ NLPÂ methods,Â beÂ theyÂ classicÂ orÂ modern,Â beginÂ withÂ aÂ textÂ dataset,Â alsoÂ calledÂ aÂ corpusÂ (plural: corpora)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A corpus is a representative sample of actual language production within a meaningful context and with a general purpose. \n",
    "\n",
    "\n",
    "> A dataset is a representative sample of a specific linguistic phenomenon in a restricted context and with annotations that relate to a specific research question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both terms are use interchangebly in the NLP literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"corpus_dataset.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TheÂ metadataÂ couldÂ beÂ anyÂ auxiliaryÂ pieceÂ ofÂ informationÂ associatedÂ withÂ theÂ text,Â likeÂ identifiers, labels,Â andÂ timestamps InÂ machineÂ learningÂ parlance,Â theÂ textÂ alongÂ withÂ itsÂ metadataÂ isÂ calledÂ an instanceÂ orÂ dataÂ point.Â \n",
    "\n",
    "- WeÂ freelyÂ interchangeÂ theÂ termsÂ corpusÂ and datasetÂ throughout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"corpus.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A General NLP Pipeline\n",
    "\n",
    "Varations of the NLP Pipelines\n",
    "\n",
    "- The process may not always be linear.\n",
    "  \n",
    "- There are loops in between.\n",
    "\n",
    "- These procedures may depend on specific task at hand.\n",
    "\n",
    "\n",
    "\n",
    "![](./nlp_pipe_line.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "- Ideal Setting: We have everything needed.\n",
    "\n",
    "- Labels and Annotations\n",
    "\n",
    "- Very often we are dealing with less-than-ideal scenario (scrape the data, public datasets)\n",
    "\n",
    "- Initial datasets with limited annotations/labels (one solution: data augmentation)\n",
    "\n",
    "**Data augmentation :** It is a technique to exploit language properties to create texts that are syntactically similar to the source text data.\n",
    "Types of strategies:\n",
    "\n",
    " - synonym replacement\n",
    " \n",
    " - Related word replacement (based on association metrics)\n",
    " \n",
    " - Back translation\n",
    " \n",
    " - Replacing entities\n",
    " \n",
    " - Adding noise to data (e.g. spelling errors, random words) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- Relevant vs. irrelevant information\n",
    "\n",
    "- non-textual information\n",
    "\n",
    "- markup\n",
    "\n",
    "- metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I feel really ğŸ˜¡. GOGOGO!! ğŸ’ªğŸ’ªğŸ’ª  ğŸ¤£ğŸ¤£ È€Ã†ÄÇ¦Æ“\n",
      "b'I feel really \\xf0\\x9f\\x98\\xa1. GOGOGO!! \\xf0\\x9f\\x92\\xaa\\xf0\\x9f\\x92\\xaa\\xf0\\x9f\\x92\\xaa  \\xf0\\x9f\\xa4\\xa3\\xf0\\x9f\\xa4\\xa3 \\xc8\\x80\\xc3\\x86\\xc4\\x8e\\xc7\\xa6\\xc6\\x93'\n"
     ]
    }
   ],
   "source": [
    "text = 'I feel really ğŸ˜¡. GOGOGO!! ğŸ’ªğŸ’ªğŸ’ª  ğŸ¤£ğŸ¤£ È€Ã†ÄÇ¦Æ“'\n",
    "print(text)\n",
    "text2 = text.encode('utf-8') # encode the strings in bytes\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I feel really . GOGOGO!!    ADG'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Preliminaries\n",
    "\n",
    "- Sentence segmentation\n",
    "\n",
    "- Word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TheÂ processÂ ofÂ breakingÂ aÂ textÂ downÂ intoÂ tokensÂ isÂ calledÂ tokenization.Â ForÂ example,Â thereÂ areÂ six tokensÂ inÂ theÂ EsperantoÂ sentenceÂ â€œMariaÂ frapisÂ laÂ verdaÂ sorÄ‰istino.â€Â \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tokenization.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tokenization_general.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](\"./tokenization_example.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./tokenization_example.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python is an interpreted, high-level and general-purpose programming language.\n",
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', 'and', 'general-purpose', 'programming', 'language', '.']\n",
      "Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\n",
      "['Python', \"'s\", 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'with', 'its', 'notable', 'use', 'of', 'significant', 'whitespace', '.']\n",
      "Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
      "['Its', 'language', 'constructs', 'and', 'object-oriented', 'approach', 'aim', 'to', 'help', 'programmers', 'write', 'clear', ',', 'logical', 'code', 'for', 'small', 'and', 'large-scale', 'projects', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = '''\n",
    "Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\n",
    "'''\n",
    "\n",
    "## sent segmentation\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "## word tokenization\n",
    "for sent in sents:\n",
    "    print(sent)\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Frequent preprocessing\n",
    "\n",
    "- Stopword removal\n",
    "\n",
    "- Stemming and/or lemmatization\n",
    "\n",
    "- Digits/Punctuaions removal\n",
    "\n",
    "- Case normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords, punctuations, digitsÂ¶\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shmuhammad/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'John', \"O'Neil\", 'works', 'at', 'Wonderland', ',', 'located', 'at', '245', 'Goleta', 'Avenue', ',', 'CA.', ',', '74208', '.']\n",
      "Mr.\n",
      "John\n",
      "O'Neil\n",
      "works\n",
      "Wonderland\n",
      "located\n",
      "Goleta\n",
      "Avenue\n",
      "CA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "text = \"Mr. John O'Neil works at Wonderland, located at 245 Goleta Avenue, CA., 74208.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(words)\n",
    "\n",
    "# remove stopwords, punctuations, digits\n",
    "for w in words:\n",
    "    if w not in eng_stopwords and w not in punctuation and not w.isdigit():\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmas and Stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " LemmasÂ areÂ rootÂ formsÂ ofÂ words.Â ConsiderÂ theÂ verbÂ fly.Â ItÂ canÂ beÂ inflectedÂ intoÂ manyÂ differentÂ words â€”flow,Â flew,Â flies,Â flown,Â flowing,Â andÂ soÂ onâ€”andÂ **fly** isÂ theÂ lemmaÂ forÂ allÂ ofÂ theseÂ seeminglyÂ different words.Â \n",
    "\n",
    "#### Stemming Algorithm\n",
    "\n",
    "- Stemming Algorithm:  algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. \n",
    "\n",
    "- This indiscriminate cutting can be successful in some occasions, but not always, and that is why we affirm that this approach presents some limitations\n",
    "\n",
    "#### Lemmatization Algorithm\n",
    "\n",
    "- Lemmatization algorithm, on the other hand, takes into consideration the morphological analysis of the words. \n",
    "\n",
    "- To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link the form back to its lemma.\n",
    "\n",
    "\n",
    "Sometimes,Â itÂ mightÂ beÂ usefulÂ toÂ reduceÂ theÂ tokensÂ toÂ theirÂ lemmasÂ toÂ keepÂ theÂ dimensionality ofÂ theÂ vectorÂ representationÂ low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./lemma.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> StemmingÂ isÂ theÂ poorÂ­manâ€™sÂ lemmatization.Â ItÂ involvesÂ theÂ useÂ ofÂ handcraftedÂ rulesÂ toÂ stripÂ endings  ofÂ wordsÂ toÂ reduceÂ themÂ toÂ aÂ commonÂ formÂ calledÂ stems.Â Â "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PopularÂ stemmersÂ oftenÂ implementedÂ inÂ open sourceÂ packagesÂ includeÂ theÂ PorterÂ andÂ SnowballÂ stemmers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Stemming\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'revolut', 'better']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = ['cars','revolution', 'better']\n",
    "print([stemmer.stem(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "revolution\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "## Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "## Wordnet requires POS of words\n",
    "poss = ['n','n','a']\n",
    "\n",
    "for w,p in zip(words,poss):\n",
    "    print(lemmatizer.lemmatize(w, pos=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Reminders for Preprocessing\n",
    "\n",
    "- Not all steps are necessary\n",
    "\n",
    "- These steps are NOT sequential\n",
    "\n",
    "- These steps are task-dependent\n",
    "\n",
    "Goals\n",
    "\n",
    "- Text Normalization\n",
    "\n",
    "- Text Tokenization\n",
    "\n",
    "- Text Enrichment/Annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TheÂ processÂ ofÂ breakingÂ aÂ textÂ downÂ intoÂ tokensÂ isÂ calledÂ tokenization.Â ForÂ example,Â thereÂ areÂ six tokensÂ inÂ theÂ EsperantoÂ sentenceÂ â€œMariaÂ frapisÂ laÂ verdaÂ sorÄ‰istino.â€Â \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blank tokenizer : Let's, go, to, N.Y., \n",
      "Default tokenizer : Let, 's, go, to, N.Y., "
     ]
    }
   ],
   "source": [
    "# Construction 1\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Creating a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "tokens = tokenizer(\"Let's go to N.Y.\")\n",
    "print(\"Blank tokenizer\",end=\" : \")\n",
    "for token in tokens:\n",
    "    print(token,end=', ')\n",
    " \n",
    "# Construction 2\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Creating a Tokenizer with the default settings for English\n",
    "tokenizer = nlp.tokenizer\n",
    "tokens = tokenizer(\"Let's go to N.Y.\")\n",
    "print(\"\\nDefault tokenizer\",end=' : ')\n",
    "for token in tokens:\n",
    "    print(token,end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TokenizationÂ canÂ becomeÂ more complicatedÂ thanÂ simplyÂ splittingÂ textÂ basedÂ onÂ nonalphanumericÂ characters,Â \n",
    "\n",
    "- ForÂ agglutinativeÂ languagesÂ likeÂ Turkish,Â splittingÂ onÂ whitespaceÂ andÂ punctuationÂ might notÂ beÂ sufficient andÂ moreÂ specializedÂ techniquesÂ mightÂ beÂ needed (chap 5 and 6).\n",
    "\n",
    "- ItÂ mayÂ beÂ possibleÂ toÂ entirelyÂ circumventÂ theÂ issueÂ ofÂ tokenizationÂ inÂ someÂ neuralÂ network modelsÂ byÂ representingÂ textÂ asÂ aÂ streamÂ ofÂ bytes;Â thisÂ becomesÂ veryÂ importantÂ forÂ agglutinative languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](chapters/Chapter_2/aglunative_hungarian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feauture Engineering\n",
    "\n",
    "- It refers to a process to feed the extracted and preprocessed texts into a machine-learning algorithm.\n",
    "\n",
    "- It aims at capturing the characteristics of the text into a numeric vector that can be understood by the ML algorithms. \n",
    "  \n",
    "- In short, it concerns how to meaningfully represent texts quantitatively, i.e., text representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for Classical ML\n",
    "\n",
    "- Word-based frequency lists\n",
    "\n",
    "- Bag-of-words representations\n",
    "\n",
    "- Domain-specific word frequency lists\n",
    "\n",
    "- Handcrafted features based on domain-specific knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams, Bigrams, Trigrams, ..., N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NÂ­gramsÂ areÂ fixedÂ­lengthÂ (n)Â consecutiveÂ tokenÂ sequencesÂ occurringÂ inÂ theÂ text.Â AÂ bigramÂ hasÂ two tokens,Â aÂ unigramÂ one\n",
    "\n",
    "- GeneratingÂ nÂ­gramsÂ fromÂ aÂ textÂ isÂ straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "\n",
    "My_text = 'Jack is very good in mathematics but he is not that much good in science'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram  ['Jack', 'is', 'very', 'good', 'in', 'mathematics', 'but', 'he', 'is', 'not', 'that', 'much', 'good', 'in', 'science'] \n",
      "\n",
      "2-gram  ['Jack is', 'is very', 'very good', 'good in', 'in mathematics', 'mathematics but', 'but he', 'he is', 'is not', 'not that', 'that much', 'much good', 'good in', 'in science'] \n",
      "\n",
      "3-gram:  ['Jack is very', 'is very good', 'very good in', 'good in mathematics', 'in mathematics but', 'mathematics but he', 'but he is', 'he is not', 'is not that', 'not that much', 'that much good', 'much good in', 'good in science'] \n",
      "\n",
      "4-gram:  ['Jack is very good', 'is very good in', 'very good in mathematics', 'good in mathematics but', 'in mathematics but he', 'mathematics but he is', 'but he is not', 'he is not that', 'is not that much', 'not that much good', 'that much good in', 'much good in science'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram \", extract_ngrams(My_text, 1), '\\n')\n",
    "print(\"2-gram \", extract_ngrams(My_text, 2), '\\n')\n",
    "print(\"3-gram: \", extract_ngrams(My_text, 3), '\\n')\n",
    "print(\"4-gram: \", extract_ngrams(My_text, 4), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pos Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['learn', 'php', 'from', 'guru99', 'and', 'make', 'study', 'easy']\n",
      "After Token: [('learn', 'JJ'), ('php', 'NN'), ('from', 'IN'), ('guru99', 'NN'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "text =\"learn php from guru99 and make study easy\".split()\n",
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for DL\n",
    "\n",
    "- DL directly takes the texts as inputs to the model.\n",
    "\n",
    "- The DL model is capable of learning features from the texts (e.g., embeddings)\n",
    "\n",
    "-  The price is that the model is often less interpretable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "## From Simple to Complex\n",
    "\n",
    "Start with heuristics or rules\n",
    "Experiment with different ML models\n",
    "\n",
    "- From heuristics to features\n",
    "  \n",
    "- From manual annotation to automatic extraction\n",
    "\n",
    "Find the most optimal model\n",
    "\n",
    "- Ensemble and stacking\n",
    "  \n",
    "- Redo feature engineering\n",
    "  \n",
    "- Transfer learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "1. NLTK Tutorials: https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](chapters/Chapter_2/aglunativelanguage.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('datascience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d203a7fbe37afbb990fedfc21c321928443618f3d7b991e0237ff71906aa031f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
